-----------FOR NFS------------------

ON MASTER:
    2  iptables -F
    3  systemctl stop firewalld.service 
    4  systemctl disable firewalld.service 
    5  systemctl status firewalld.service 
    6  vi /etc/selinux/config 
    7  hostnamectl set-hostname server
    8  exec bash
    9  init 6
   10  ip a
   11  yum update -y
   12  yum install -y nfs-utils
   13  systemctl start nfs-server rpcbind
   14  systemctl enable nfs-server rpcbind
 
   16  mkdir home/
   17  chmod 777 /home/
   18  vi /etc/exports
   19  exportfs -r
   20  ls /home
         test
   21. unmount ..

---------FOR SLURM------------
   96    yum install epel-release -y
   97  yum install munge munge-libs munge-devel -y
   98  ll /etc/munge
   99  
 
  101  create-munge-key -r
  102  scp /etc/munge/munge.key client1:/etc/munge/
  103  scp /etc/munge/munge.key client2:/etc/munge/
  104  systemctl restart munge.service 
  105  systemctl status munge.service
       chown munge:munge /etc/munge/ 
  106  wget https://download.schedmd.com/slurm/slurm-20.11.9.tar.bz2
  107  yum install rpm-build
  108  rpmbuild -ta slurm-20.11.9.tar.bz2 
  109  yum install python3 readline-devel perl-ExtUtils-MakeMaker -y
  110  yum install python3 readline-devel perl-ExtUtils-MakeMaker mysql-devel -y
  111  yum install gcc -y
  112  rpmbuild -ta slurm-20.11.9.tar.bz2 
  113  yum install pam-devel 
  114  rpmbuild -ta slurm-20.11.9.tar.bz2 
  116   ls /root/rpmbuild/RPMS/x86_64/
  117  mkdir /home/rpms
  118  cd /root/rpmbuild/RPMS/x86_64/
  119  cp * /home/rpms/
  120  ls
  121  history
  122  cd
  123  cd /home/rpms/
  124  ls
  125  yum --nogpgcheck local install * -y
  126  yum --nogpgcheck localinstall * -y
  127  history
  128  pwd
  129  cd ..
  130  ls
  131  scp rpms client1:/root
  132  scp -r rpms client1:/root
  133  scp -r rpms client2:/root
  134  rpm -qa | grep slurm | wc -l
       export SLURMUSER=900
       groupadd -g $SLURMUSER slurm
       useradd -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm -s /bin/bash slurm
  135  cd
  136  mkdir /var/spool/slurm
  137  ll /var/spool/slurm
  138  chown slurm:slurm /var/spool/slurm
  139  ll /var/spool/
  140  mkdir /var/log/slurm
  141  chown -R slurm . /var/log/slurm
  142  touch /var/log/slurm/slurmctld.log
  143  chown slurm:slurm /var/log/slurm/slurmctld.log
  144  touch /var/log/slurm/slurm_jobacct.log /var/log/slurm_jobcomp.log
  145  chown slurm: /var/log/slurm/slurm_jobacct.log /var/log/slurm_jobcomp.log
  146  cp /etc/slurm/slurm.conf.example /etc/slurm/slurm.conf
  147  vi etc/slurm/slurm.conf
  148  vi /etc/slurm/slurm.conf
       ClusterName=HPCSA
       ControlMachine=server
       #ControlAddr=
       #BackupController=
       #BackupAddr=
         +++++
       # COMPUTE NODES
       NodeName=linux[1-32] Procs=1 State=UNKNOWN
       NodeName=client1 CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=3770 State=UNKNOWN
       NodeName=client2 CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=3770 State=UNKNOWN
       PartitionName=standard Nodes=ALL Default=YES MaxTime=INFINITE State=UP
      
  149  scp /etc/slurm/slurm.conf client1:/etc/slurm/
  150  scp /etc/slurm/slurm.conf client2:/etc/slurm/
  151  systemctl restart slurmctld.service 
  152  systemctl enable slurmctld.service 

---------------------FOR NFS------------------
ON CLIENT1 and CLIENT2:
1  hostnamectl set-hostname client1
    2  exec bash
    3  yum update -y
    4  kill -9 10069
    5  vi /etc/selinux/config 
    6  systemctl stop firewalld.service
    7  systemctl disable firewalld.service
    8  systemctl status firewalld.service
    9  init 6
   10  yum install -y nfs-utils
  
   12  showmount -e 192.168.20.186 {SERVER NAT IP}
   13  mkdir /mnt/home
   14  mount 192.168.20.186:/home /mnt/home
   15  mount | grep nfs
   16  df -hT
   17. touch /mnt/home/test
   18  ls /mnt/home
         test
   17  vi /etc/fstab
        192.168.1.10:/home /mnt/home    nfs     nosuid,rw,sync,hard,intr  0  0
   18  reboot
   19  df -hT
   20  mount | grep nfs
   21 unmount 

-------FOR SLURM---------------------

   55  yum install munge munge-libs munge-devel -y
   54  cd /etc/munge
   55  ls
   56  systemctl restart munge.service
   57  chown munge:munge /etc/munge/munge.key 
   58  systemctl restart munge.service
   59  yum install pam-devel python3 readline-devel perl-ExtUtils-MakeMaker mysql-devel -y
   60  export SLURMUSER=900
       groupadd -g $SLURMUSER slurm
   61  useradd -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm -s /bin/bash slurm
   69  cd /home/rpms
   70  mkdir /home/rpms
   71  cd /home/rpms/
   72  ll
   73  dh -th
   74  df -th
   75  ls /mnt/home
   76  ll
   77  history
   78  mount | grep nfs
   79  df -th
   80  df -Th
   81  cd
   82  ll /root
   83  cd rpms
   84  ll
   85  rm -rf slurm-slurmctld-20.11.9-1.el7.x86_64.rpm 
   86  cd
   
   88  yum --nogpgcheck localinstall * -y
   89  rpm -qa | grep slurm | wc -l
   90  ls
   91  cd rpms
   92  ls
   93  yum install --nogpgcheck localinstall * -y
   94  rpm -qa | grep slurm | wc -l
   95  rpm -e slurm-slurmdbd
   96  ls
   97  rpm -qa | grep slurm | wc -l
   98  mkdir /var/spool/slurm
   99  cd
  100  chown slurm:slurm /var/spool/slurm
  101  mkdir /var/log/slurm
  102  chown -R slurm . /var/log/slurm
  103  slurmd -C [TO SEE THE STATUS OF NODES]
  104  systemctl restart slurmd.service 
  105  systemctl enable slurmd.service 
===================================================	DAY2 ===========================

ON MASTER:

   91  systemctl restart slurmctld.service
   92  systemctl status slurmctld.service
   93  sinfo
   94  systemctl restart munge.service
   95  systemctl statud munge.service
   96  systemctl status munge.service
   97  sinfo
   98  scontrol update node=client1 state=idle
   99  scontrol update node=client2 state=idle
  [root@server ~]# sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
standard*    up   infinite      2  idle client[1-2]
   100  slurmctld -Dvv  {TO CHECK THE LOG OR DEIUG}
   scp /etc/slurm/slurm.conf client:/etc/slurm/
  102  sinfo
  103  srun -w client --pty /bin/bash
  104  vim demo.sh
  105  sbatch demo.sh
  106  squeue
  107  sinfo
  108  squeue
  109  sinfo
  110  scontrol show job 3
  111  sshare
  112  yum install mariadb-server mariadb-devel -y
  113  systemctl enable mariadb
  114  systemctl start mariadb
  115  systemctl status mariadb
  116  mysql
  117  mysql -p -u slurm
  118  cd /etc/my.cnf.d/
  119  ll
  120  vim innodb.cnf
  121  cd
  122  systemctl stop mariadb
  123  ll /var/lib/
  124  ll /var/lib/mysql/
  125  mv /var/lib/mysql/ib_logfile0 /tmp/
  126  systemctl start mariadb
  127  journalctl -xe
  128  vi /etc/my.cnf
  129  yum autoremove mariadb*
  130  yum reinstall mariadb*
  131  systemctl restart mariadb
  132  systemctl status mariadb
  133  cd /var/lib/
  134  ll
  135  cd m
  136  cd mysql/
  137  ll
  138  mkdir back
  139  mv aria_log* ibdata1 ib_logfile* back/
  140  ls
  141  systemctl restart mariadb
  142  systemctl status mariadb
  143  systemctl enable mariadb
  144  sacct
  145  systemctl status slurmdbd
  146  cd 
  147  mysql
  148  ll /etc/slurm/
  149  vim /etc/slurm/slurmdbd.conf
  150  chown slurm: /etc/slurm/slurmdbd.conf
  151  chmod 600 /etc/slurm/slurmdbd.conf
  152  touch /var/log/slurmdbd.log
  153  chown slurm: /var/log/slurmdbd.log
  154  slurmdbd -D -vvv
	systemctl enable slurmdbd
	systemctl start slurmdbd
	systemctl status slurmdbd
	systemctl enable slurmctld.service
	systemctl start slurmctld.service
	systemctl status slurmctld.service

ON CLIENT1 & CLIENT2:

   39  systemctl restart munge.service
   40  systemctl status munge.service
   41  sinfo
   42  hostname
   43  systemctl restart slurmd.service
   44  systemctl status slurmd.service
   45  slurmd -Dvv  {TO CHECK THE LOG OR DEIUG}
  




















































   
   
